{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bd73053-73d9-4b18-b009-bb597a23f3ab",
   "metadata": {},
   "source": [
    "# Dask-SQL: Empowering Pythonistas for <br>Scalable End-to-End Data Engineering and Data Science\n",
    "\n",
    "<img src='images/med-head.jpg' width=300>\n",
    "\n",
    "## Who Am I?\n",
    "\n",
    "### Adam Breindel\n",
    "\n",
    "__LinkedIn__ - https://www.linkedin.com/in/adbreind<br>\n",
    "__Email__ - <tt>adbreind@gmail.com</tt><br>\n",
    "__Twitter__ - <tt>@adbreind</tt>\n",
    "\n",
    "__What Do I Do?__\n",
    "* Training Lead at Coiled Computing: https://coiled.io\n",
    "    * Dask scales Python for data science and machine learning\n",
    "    * Coiled makes it easy to scale on the cloud\n",
    "* Consulting on data engineering and machine learning\n",
    "    * Development\n",
    "    * Various advisory roles\n",
    "* 20+ years building systems for startups and large enterprises\n",
    "* 10+ years teaching front- and back-end technology\n",
    "\n",
    "__Fun large-scale data projects__\n",
    "* Streaming neural net + decision tree fraud scoring\n",
    "* Realtime & offline analytics for banking\n",
    "* Music synchronization and licensing for networked jukeboxes\n",
    "\n",
    "__Industries__\n",
    "* Finance / Insurance\n",
    "* Travel, Media / Entertainment\n",
    "* Energy, Government\n",
    "* Advertising/Social Media, & more"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5395ad04-4446-493a-a51f-3cceef4d40f5",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Basic large-scale enterprise data processing pattern\n",
    "\n",
    "<img src='images/end2end.png' width=800>\n",
    "<br>\n",
    "<br>\n",
    "Yes, we're missing a lot of important upstream work (data aquisition, ingestion) and downstream (deploy, monitor), but today we're focusing on *SQL*\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Let's zoom in on extracting from a data lake/warehouse and transforming\n",
    "\n",
    "<img src='images/2020.png' width=800>\n",
    "\n",
    "* There are __other__ tools (Presto/Trino, Spark, etc.) that can help\n",
    "* But we're *Pythonistas* and maybe not experts (or interested) in integrating complex JVM-based tools\n",
    "* And we'd like to ...\n",
    "  * Use Python together with SQL at scale\n",
    "  * Create services and tools for our company/team that use SQL\n",
    "  * Because many more folks know SQL than Python! (I know it's hard to believe, but it's true :)\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# We're all happy it's 2021\n",
    "\n",
    "<img src='images/2021.png' width=800>\n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cfd79c-1234-4d2a-83c5-d0bf633b3949",
   "metadata": {},
   "source": [
    "# Introducing Dask-SQL\n",
    "## Adding SQL execution and Hive access to Python!\n",
    "\n",
    "<img src='images/nils.jpeg' width=300>\n",
    "\n",
    "### Nils Braun\n",
    "* Data Engineer for Enabling: Bosch Center for Artificial Intelligence (BCAI)\n",
    "* https://www.linkedin.com/in/nlb/\n",
    "* https://github.com/nils-braun\n",
    "\n",
    "### Dask-SQL\n",
    "\n",
    "Core features\n",
    "\n",
    "* SQL parsing, optimization, planning, translation for Dask\n",
    "* Start with data from...\n",
    "    * files in the cloud (e.g., S3)\n",
    "    * any data in Python (e.g., Pandas or Dask Dataframe)\n",
    "    * modern data catalog/aggregation like Intake (https://github.com/intake/intake)\n",
    "    * __direct from enterprise data lakes/warehouses: Hive Metastore, Databricks, etc.__\n",
    "        * Bring the SQL integration power of Spark right into the Python/Dask world\n",
    "* Query cached datasets to leverage the speed of a large distributed memory pool\n",
    "\n",
    "Bonus features\n",
    "* user-defined functions\n",
    "* a SQL server\n",
    "* ML in SQL\n",
    "* a command-line client\n",
    "* more in the works!\n",
    "\n",
    "Learn more...\n",
    "* Homepage: https://nils-braun.github.io/dask-sql/\n",
    "* Docs: https://dask-sql.readthedocs.io/en/latest/\n",
    "* Source: https://github.com/nils-braun/dask-sql\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962139c9-72a8-4c1f-bba1-f44f6056776f",
   "metadata": {},
   "source": [
    "## Before we dive into code ... a little clarification: data lakes\n",
    "\n",
    "If you haven't worked a lot in the large-scale data space, it can be a bit confusing why we need a Dask-SQL project. Common questions include...\n",
    "\n",
    "How is this different from...\n",
    "* Dask `read_sql_table`? \n",
    "* Pandas `read_sql`, `read_sql_table`, or `read_sql_query`?\n",
    "* SQLAlchemy\n",
    "* etc.\n",
    "\n",
    "The fundamental difference is: __those other approaches pass your query to a database system which already understands SQL, can execute a query, and has control over your data__\n",
    "\n",
    "__In enterprise data lakes, that \"database\" likely does not exist.__ Instead, you may have huge collections of files, in a variety of formats, with no query engine, and no process which has \"control\" over your data.\n",
    "\n",
    "You may not even have a data catalog. In other cases, you may have a catalog, but it is tied to a Hadoop/JVM-based system like Hive or Spark.\n",
    "\n",
    "In these data lake systems, all of the `read_sql` techniques above may not work at all, or may require you to pass your logic through to Hive/Spark/etc., requiring you to understand, use, and tune those systems before you can even start your work in Python.\n",
    "\n",
    "The goal of Dask-SQL is to allow you to formulate a SQL query against arbitrary files & formats, and execute that query at large scale with Dask."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4de9c3-c9a8-4d3e-a073-8ba439fcb807",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "## It's coding time!\n",
    "\n",
    "We'll demo three key approaches here:\n",
    "\n",
    "1. Creating a Dask Dataframe -- a lazy, distributed datastructure -- over a set of files, and then using Dask-SQL to query the data\n",
    "\n",
    "2. Creating a Dask-SQL table completely within SQL, and querying that -- an approach that will be very helpful working your SQL analyst friends\n",
    "\n",
    "3. Using Dask-SQL to access tables *already defined in the Hive catalog (\"metastore\")* but querying the underlying files with Dask -- an incredibly valuable missing link for Python data folks working within orgs that rely on Hive to catalog their data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd2fd9-4793-4f1a-a7e8-b3740442e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c9e855f-3cb5-4764-ab85-e5eb47c0373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_sql import Context\n",
    "\n",
    "c = Context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667aee4b-6c9c-42af-a07e-ea2b123c64aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_csv('data/powerplant.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da3e6fb-ecef-48cd-8d97-cbc6c2fcaa99",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.create_table(\"powerplant\", df)\n",
    "\n",
    "result = c.sql('SELECT * FROM powerplant')\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c168bfd-2ae3-44e4-bc43-bc960ecce869",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec06cec-b204-44f6-9026-2f82a3aa8d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd9abde4-e6e0-4ed9-8251-69c8652639c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.sql('SELECT * FROM powerplant', return_futures=False) # run immediately -- beware of large result sets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59a600c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(c.sql('SELECT * FROM powerplant', return_futures=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce616b95-11a9-4631-a374-f9901632089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT\n",
    "    FLOOR(\"AT\") AS temp, AVG(\"PE\") AS output\n",
    "FROM\n",
    "    powerplant\n",
    "GROUP BY \n",
    "    FLOOR(\"AT\")\n",
    "'''\n",
    "\n",
    "result = c.sql(query)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146351e1-7b08-4156-af87-2452e0c89827",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.compute().plot.scatter('temp','output')\n",
    "\n",
    "# hint: if you're not totally convinced the computation is happening in Dask, look at the Dask Task Stream dashboard!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b750332-7c6b-4352-a138-66e9777ca021",
   "metadata": {},
   "source": [
    "Maybe we could build a successful model with this data ... in fact, we could do it with any combination of\n",
    "* Data prep in SQL, training/prediction in Python\n",
    "* Training in Python, prediction in SQL\n",
    "* Everything (!) in SQL\n",
    "* Sound interesting? Check it out: https://dask-sql.readthedocs.io/en/latest/pages/machine_learning.html\n",
    "\n",
    "### What about \"creating the table completely in SQL\"?\n",
    "\n",
    "First, let's go \"full SQL\" so we don't even need to wrap our queries in Python..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50f01d0-5753-4f5e-91c4-9768211f77f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.ipython_magic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2f680-18af-49a5-b767-f7ba6adf1e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "CREATE TABLE allsql WITH (\n",
    "    format = 'csv',\n",
    "    location = 'data/powerplant.csv' -- any Dask-accessible source or format (cloud/S3/..., parquet/ORC/...)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189d158",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT\n",
    "    FLOOR(\"AT\") AS temp, AVG(\"PE\") AS output\n",
    "FROM\n",
    "    allsql\n",
    "GROUP BY \n",
    "    FLOOR(\"AT\")\n",
    "LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c0b6a7",
   "metadata": {},
   "source": [
    "### Let's see that Hive catalog integration!\n",
    "\n",
    "*note: this demo will not run in the standalone binder notebook available after PyCon, as it relies on a Hive server which is not configured in that container*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c27141",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive.hive import connect\n",
    "\n",
    "cursor = connect(\"localhost\", 10000).cursor()\n",
    "\n",
    "c.create_table(\"my_diamonds\", cursor, hive_table_name=\"diamonds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f9c10a",
   "metadata": {},
   "source": [
    "Here's the magic...\n",
    "* If you look at the Hive Server web UI, you'll see a query just ran to get schema info on the `Diamonds` table\n",
    "* But in the following queries\n",
    "    * Data is accessed directly from the underlying files\n",
    "    * No Hive queries are run\n",
    "    * All compute is done in Dask/Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404722db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "\n",
    "SELECT * FROM my_diamonds LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7de52bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "SELECT FLOOR(10*carat)/10 AS carat, AVG(price) AS price, COUNT(1) AS num \n",
    "FROM my_diamonds\n",
    "GROUP BY FLOOR(10*carat)\n",
    "'''\n",
    "\n",
    "data = c.sql(query).compute()\n",
    "\n",
    "data.plot.scatter('carat', 'price')\n",
    "data.plot.bar('carat', 'num')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99da79e-953e-4eb4-b213-82875016cc31",
   "metadata": {},
   "source": [
    "## A Quick Look at How Dask-SQL Works\n",
    "\n",
    "* Locate the source data\n",
    "    * Hive, Intake, Databricks catalog integration\n",
    "    * Files or Python data provided by user\n",
    "\n",
    "\n",
    "* Prepare the query using Apache Calcite\n",
    "    * Parse SQL\n",
    "    * Analyze (check vs. schema, etc.)\n",
    "    * Optimize\n",
    "\n",
    "\n",
    "* Create execution plan\n",
    "    * Take logical relational operators (`SELECT`/project, `WHERE`/filter, `JOIN`, etc.) \n",
    "    * Convert into Dask Dataframe API calls (`query`, `merge`, etc.)\n",
    "\n",
    "\n",
    "* Then either...\n",
    "    * Return a handle to the Dask Dataframe of results (recall this is a virtual Dataframe, so no execution yet)\n",
    "    * or\n",
    "    * Compute (materialize) the resulting dataframe and return the result as a Pandas Dataframe\n",
    "    \n",
    "More detail at https://dask-sql.readthedocs.io/en/latest/pages/how_does_it_work.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f560c5e3-1688-4178-bb77-5cb6864d4b25",
   "metadata": {},
   "source": [
    "## Some Practical Details\n",
    "\n",
    "### Installing Dask-SQL\n",
    "\n",
    "Recommended approach is via conda and conda-forge -- this will include all dependencies like the JVM, and avoid conflicts by keeping everything within a conda environment.\n",
    "\n",
    "There are also a few other options: more details at https://dask-sql.readthedocs.io/en/latest/pages/installation.html\n",
    "\n",
    "### Supported SQL Operators\n",
    "\n",
    "Dask-SQL is a young project, so it does not yet support all of SQL\n",
    "\n",
    "More detail on\n",
    "* Query support https://dask-sql.readthedocs.io/en/latest/pages/sql/select.html\n",
    "* Table creation https://dask-sql.readthedocs.io/en/latest/pages/sql/creation.html\n",
    "* ML via SQL https://dask-sql.readthedocs.io/en/latest/pages/sql/ml.html\n",
    "\n",
    "### How to Contribute\n",
    "\n",
    "Source code and info on installing for development is at  https://github.com/nils-braun/dask-sql\n",
    "\n",
    "Check issues -- or file a new bug -- at https://github.com/nils-braun/dask-sql/issues\n",
    "\n",
    "And there's even a \"good first issue\" list at https://github.com/nils-braun/dask-sql/contribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7488066",
   "metadata": {},
   "source": [
    "# Thank You!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4999eff9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
